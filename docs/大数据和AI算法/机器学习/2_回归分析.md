## 2.1 引言

在机器学习方法中，回归分析是一种预测性的建模技术，它研究因变量（目标特征）和自变量（特征）之间的关系。这种技术通常用于预测分析、时间序列模型以及发现特征之间的因果关系。

## 2.2 一元线性回归

一元线性回归模型：y=ax+b，x为自变量，y为因变量，a为系数是斜率，b是截距。

> 线性回归，就是能够用一条直线较为精确地描述数据之间的关系 。

不同的a和b，可做出不同的线，找出更能完美体现x和y对应关系的线的过程，就是确定模型参数的过程。

![image-20230822223051657](https://raw.githubusercontent.com/AnJian2020/study_recorder/main/images/202308222230750.png)

重写y=ax+b为h=θ_0+θ_1x，简化问题，减少参数的数量=> h = θ*x

**MSE**：均方误差（误差平方和的均值）
$$
J = {1\over{m}}{\sum_{i=0}^{m-1}(h_i-y_i)^2}
$$
误差平方和的均值用一个符号J表示，J值为m个样本的误差平方和的均值，**hi**为第i个样本的预测值，**yi**为第i个样本的真实值。

J我们称为代价函数/误差函数。

![image-20230822223754668](https://raw.githubusercontent.com/AnJian2020/study_recorder/main/images/202308222237737.png)

- 当θ值在θ最优的右侧时，能够不停的向左迭代，靠近θ最优。
- 当θ值在θ最优的左侧时，能够不停的向右迭代，靠近θ最优。

![image-20230822223908962](https://raw.githubusercontent.com/AnJian2020/study_recorder/main/images/202308222239022.png)

相关系数是用以反映变量之间相关关系密切程度的统计指标。
$$
r(相关系数)={cov(x,y)\over{σ_x*σ_y}}
$$
对于相关性强度r来说有以下的关系：

- 0~0.3 弱相关
- 0.3~0.6 中等程度相关
- 0.6~1 强相关

均方误差 =  Σ(y实际值 - y预测值)^2 /样本数

方差 = Σ(y实际值 - y平均值)^2，表示y的总波动

Score：决定系数R平方= 1 – SSE/方差 

- 回归线拟合程度：有多少百分比的y波动被回归线来描述(x的波动变化)
- 值大小：R平方越高，回归模型越精确(取值范围0~1)，1无误差，0无法完成拟合，对于预测来说我们需要运用函数中的model.predict()来得到预测值.

## 2.3 多元线性回归

对于n个特征的线性模型为
$$
f(x) = w_0+w_1*x_1+...+x_n*w_n
$$
使用矩阵来表示就是f(x)=XW,其中W是所要求取的参数（系数），X是具有n个属性（特征）的m个数据样本，该样本的真实标签为y。

![image-20230823232211142](https://raw.githubusercontent.com/AnJian2020/study_recorder/main/images/202308232322270.png)

### 2.3.1 梯度下降

$$
线性模型：f(x) = XW
$$


$$
数据集标签为 y = \begin{bmatrix}y^{(1)}\\y^{(2)}\\...\\y^{(m)}\end{bmatrix}
$$

$$
损失函数：J(w) = {1\over{m}}{\sum_{i=1}^m (f(x^{(i)})-y^{(i)})^2} = {1\over m}(XW-y)^T(XW-y)
$$

**使得J(w)最小的方法：**

1. 初始参数wi 设置（一般采用随机法）

2. $$
   更新 W \leftarrow W - \alpha * {\partial J(W) \over {\partial W}}
   \\其中\alpha是学习率
   $$

3. 直到达到全局最优解或者局部最优或者最大迭代次数

![image-20230823233959122](https://raw.githubusercontent.com/AnJian2020/study_recorder/main/images/202308232339198.png)

![image-20230823234012567](https://raw.githubusercontent.com/AnJian2020/study_recorder/main/images/202308232340652.png)

> **最小二乘法**
>
> - 全局最优解，因为一步到位，直接求极值，因而步骤简单。
> - 线性回归的模型假设，这是最小二乘方法的优越性前提，否则不能推出最小二乘是最佳（即方差最小）的无偏估计。
>
> **梯度下降法**
>
> - 局部最优解，因为是一步步迭代的，而非直接求得极值。
> - 既可以用于线性模型，也可以用于非线性模型，没有特殊的限制和假设条件。
>
> **总结**
>
> 最小二乘法通过使偏导结果等于0，从而直接求得极值，但通常要假设线性模型的误差函数服从均值为0，方差为σ的正态分布，且独立，得到的是全局最优解。
>
> 而梯度下降则不需要特殊的假设前提条件，是将推导结果带入迭代公式中，一步一步地得到最终结果，得到的是局部最优解。
>
> 简单地说，最小二乘法是一步到位的，而梯度下降是一步步进行的。

### 2.3.2 线性回归算法流程

在解决实际问题时，简单的线性回归分析的过程：

1. 确定输入变量和目标变量间的回归模型

2. 确定损失函数形式
   $$
   f(x) = w_0+w_1*x_1+...+x_n*w_n
   \\
   J(w) = {1\over{m}}{\sum_{i=1}^m (f(x^{(i)})-y^{(i)})^2}
   $$

3. 训练算法，找到回归系数，如最小二乘法、梯度下降等

4. 使用算法进行数据预测
