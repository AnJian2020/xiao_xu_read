## 2.1 引言

在机器学习方法中，回归分析是一种预测性的建模技术，它研究因变量（目标特征）和自变量（特征）之间的关系。这种技术通常用于预测分析、时间序列模型以及发现特征之间的因果关系。

## 2.2 一元线性回归

一元线性回归模型：y=ax+b，x为自变量，y为因变量，a为系数是斜率，b是截距。

> 线性回归，就是能够用一条直线较为精确地描述数据之间的关系 。

不同的a和b，可做出不同的线，找出更能完美体现x和y对应关系的线的过程，就是确定模型参数的过程。

![image-20230822223051657](https://raw.githubusercontent.com/AnJian2020/study_recorder/main/images/202308222230750.png)

重写y=ax+b为h=θ_0+θ_1x，简化问题，减少参数的数量=> h = θ*x

**MSE**：均方误差（误差平方和的均值）
$$
J = {1\over{m}}{\sum_{i=0}^{m-1}(h_i-y_i)^2}
$$
误差平方和的均值用一个符号J表示，J值为m个样本的误差平方和的均值，**hi**为第i个样本的预测值，**yi**为第i个样本的真实值。

J我们称为代价函数/误差函数。

![image-20230822223754668](https://raw.githubusercontent.com/AnJian2020/study_recorder/main/images/202308222237737.png)

- 当θ值在θ最优的右侧时，能够不停的向左迭代，靠近θ最优。
- 当θ值在θ最优的左侧时，能够不停的向右迭代，靠近θ最优。

![image-20230822223908962](https://raw.githubusercontent.com/AnJian2020/study_recorder/main/images/202308222239022.png)

相关系数是用以反映变量之间相关关系密切程度的统计指标。
$$
r(相关系数)={cov(x,y)\over{σ_x*σ_y}}
$$
对于相关性强度r来说有以下的关系：

- 0~0.3 弱相关
- 0.3~0.6 中等程度相关
- 0.6~1 强相关

均方误差 =  Σ(y实际值 - y预测值)^2 /样本数

方差 = Σ(y实际值 - y平均值)^2，表示y的总波动

Score：决定系数R平方= 1 – SSE/方差 

- 回归线拟合程度：有多少百分比的y波动被回归线来描述(x的波动变化)
- 值大小：R平方越高，回归模型越精确(取值范围0~1)，1无误差，0无法完成拟合，对于预测来说我们需要运用函数中的model.predict()来得到预测值.

## 2.3 多元线性回归

对于n个特征的线性模型为
$$
f(x) = w_0+w_1*x_1+...+x_n*w_n
$$
使用矩阵来表示就是f(x)=XW,其中W是所要求取的参数（系数），X是具有n个属性（特征）的m个数据样本，该样本的真实标签为y。

![image-20230823232211142](https://raw.githubusercontent.com/AnJian2020/study_recorder/main/images/202308232322270.png)

### 2.3.1 梯度下降

$$
线性模型：f(x) = XW
$$


$$
数据集标签为 y = \begin{bmatrix}y^{(1)}\\y^{(2)}\\...\\y^{(m)}\end{bmatrix}
$$

$$
损失函数：J(w) = {1\over{m}}{\sum_{i=1}^m (f(x^{(i)})-y^{(i)})^2} = {1\over m}(XW-y)^T(XW-y)
$$

**使得J(w)最小的方法：**

1. 初始参数wi 设置（一般采用随机法）

2. $$
   更新 W \leftarrow W - \alpha * {\partial J(W) \over {\partial W}}
   \\其中\alpha是学习率
   $$

3. 直到达到全局最优解或者局部最优或者最大迭代次数

![image-20230823233959122](https://raw.githubusercontent.com/AnJian2020/study_recorder/main/images/202308232339198.png)

![image-20230823234012567](https://raw.githubusercontent.com/AnJian2020/study_recorder/main/images/202308232340652.png)

> **最小二乘法**
>
> - 全局最优解，因为一步到位，直接求极值，因而步骤简单。
> - 线性回归的模型假设，这是最小二乘方法的优越性前提，否则不能推出最小二乘是最佳（即方差最小）的无偏估计。
>
> **梯度下降法**
>
> - 局部最优解，因为是一步步迭代的，而非直接求得极值。
> - 既可以用于线性模型，也可以用于非线性模型，没有特殊的限制和假设条件。
>
> **总结**
>
> 最小二乘法通过使偏导结果等于0，从而直接求得极值，但通常要假设线性模型的误差函数服从均值为0，方差为σ的正态分布，且独立，得到的是全局最优解。
>
> 而梯度下降则不需要特殊的假设前提条件，是将推导结果带入迭代公式中，一步一步地得到最终结果，得到的是局部最优解。
>
> 简单地说，最小二乘法是一步到位的，而梯度下降是一步步进行的。

### 2.3.2 线性回归算法流程

在解决实际问题时，简单的线性回归分析的过程：

1. 确定输入变量和目标变量间的回归模型

2. 确定损失函数形式
   $$
   f(x) = w_0+w_1*x_1+...+x_n*w_n
   \\
   J(w) = {1\over{m}}{\sum_{i=1}^m (f(x^{(i)})-y^{(i)})^2}
   $$

3. 训练算法，找到回归系数，如最小二乘法、梯度下降等

4. 使用算法进行数据预测

## 2.4 非线性回归分析

回归分析用于预测输入变量（自变量）和输出变量（因变量）之间的关系，特别是当输入变量的值发生变化时，输出变量值随之发生变化。

> 自变量个数：一元回归分析、多元回归分析
>
> 自变量与因变量关系：线性回归分析、非线性回归分析
>
> 因变量个数：简单回归分析、多重回归分析

如果回归模型的因变量是自变量的一次以上函数形式，回归规律在图形上表现为形态各异的各种曲线，称为非线性回归。

![image-20230826235510165](https://raw.githubusercontent.com/AnJian2020/study_recorder/main/images/202308262355412.png)

通常线性回归分析法是最基本的分析方法，遇到非线性回归问题可以借助数学手段化为线性回归问题处理。

也就是说，对于一些非线性函数，我们可以使用适当的转换，进行线性化。

**非线性回归转线性回归**

1. 可将部分非线性回归转化为线性回归(Linear Regression)的方式来求解非线性回归问题；
2. 部分非线性回归无法转化为线性回归但是可以转换成多项式回归(Polynomial Regression)

![image-20230827000150068](https://raw.githubusercontent.com/AnJian2020/study_recorder/main/images/202308270001195.png)

> 利用回归方程探究非线性回归问题，在低维特征空间，可按：“作散点图→建模→确定方程”这三个步骤进行。

如何通过适当的变换，将非线性回归问题转化成线性回归问题？（泰勒展开式）
$$
f(x) = f(0)+f'(0)x+{{f'(0)\over2!}x^2}+...+{{f^{(n)}(0)\over n!}x^n}+o(x^n)
$$
实现多项式回归，在sklearn中，提供了简单的方法PolynomialFeatures，为每个特征添加幂次方做为新的特征。PolynomialFeatures构造方法有三个主要的参数，第一个是degree，默认为2，是多项式中的次数，interaction_only，布尔型，表示是否只产生交互项，默认为False，include_bias布尔型，是否产出与截距项相乘的项，默认True。

```python
def __init__(self, degree=2, *, interaction_only=False, include_bias=True,  order='C'):
	self.degree = degree #多项式中的次数
	self.interaction_only = interaction_only # 是否只产生交互项
	self.include_bias = include_bias  #是否产出与截距项相乘的项
	self.order = order

```

![image-20230827000941155](https://raw.githubusercontent.com/AnJian2020/study_recorder/main/images/202308270009237.png)

**在sklearn中，多项式回归与线性回归的使用方法一致，唯一不同的是需要首先创造多项式（生成新特征），然后在这个拓展过的训练集上X_poly训练模型，使用线性回归的方法来进行拟合，成为多项式回归，也就是借助多项式回归来对数据进行非线性回归建模。**

模型复杂度与训练误差及测试误差之间的关系

<img src="https://raw.githubusercontent.com/AnJian2020/study_recorder/main/images/202308270010615.png" alt="image-20230827001047540" style="zoom:50%;" />

高次多项式可以更好地拟合数据，但同时，过高的幂次可能会显示出不必要的振荡效应，这在振荡的部分，显然很有可能超出数据的拟合范围，这是危险的。

> **过拟合是相对样本的容量而言的。**

如何解决过拟合？

1. 过拟合的突出表现就是参数估计值取了一些极大的正值或极小的负值，为了避免这种情况出现，解决过拟合问题，可以**增加样本数据**，
2. 可以**加入正则项（惩罚项）。**正则项通过在误差函数基础上添加惩罚项的方式，对那些绝对值较大的参数进行惩罚，逼迫其向0靠拢,或者说是向0收缩（shrinkage）。

**线性回归扩展**

线性回归扩展算法用简单的基函数Φ(x)替换输入变量x。这样就把线性拟合形式扩展到了固定非线性函数的线性组合。

 <img src="https://raw.githubusercontent.com/AnJian2020/study_recorder/main/images/202308270018222.png" alt="image-20230827001839133" style="zoom:67%;" />

## 2.5 正则化

过拟合：模型的训练误差很小，测试误差很大

欠拟合：模型的训练误差和测试误差都大

- 训练误差：模型在训练集上的误差
- 测试误差：训练好的模型在测试集上的误差
- 泛化误差：训练好的模型在新样本上的误差

**通常泛化误差无法直接获得，而训练误差又存在过拟合现象。**

过拟合原因：

- 模型的复杂度过高；
- 模型的输入特征数目过多；
- 样本里的噪音数据干扰过大，大到模型过分记住了噪音特征，反而忽略了真实的输入输出间的关系。

过拟合是无法彻底避免的，只能做到缓解或者说减小其风险。

解决过度拟合的方法（扩增训练集；减少训练使用的特征的数量；提高正则化水平）：

1. 减少特征的数量
   - 手动选择要保留的特征
   - 模型选择算法
2. 正则化
   - 保留所有特征，但减少参数的幅值。
   - 当我们有很多特性时，效果很好，每一个都有助于预测，如无必要，勿增实体。

通过降低复杂模型的复杂度来防止过拟合，通过惩罚模型参数的较大值，来限制模型中的尖峰，这种原则称为正则化。

最基本的正则化方法是在原目标（代价）函数 中添加惩罚项，对复杂度高的模型进行“惩罚”。其数学表达形式为

 <img src="https://raw.githubusercontent.com/AnJian2020/study_recorder/main/images/202308271810258.png" alt="image-20230827181046075" style="zoom:50%;" />

λ：惩罚系数，对参数**θ**起到限制作用。λ增大，减少过拟合，λ减小，减少欠拟合。

第一项是衡量模型（如回归模型）对第i个样本的预测值和真实值之前的误差。

第二项，也就是对参数w的规则化函数Ω(w)，去约束我们的模型尽量的简单。

λ是惩罚系数，对模型参数w起到限制作用，控制控制正则化强弱，增大λ，可以减少过拟合，反之，减小λ，可以减少欠拟合。因此，我们可以根据模型对真实数据的拟合情况调节λ。

不同的Ω(w)函数对w的最优解有不同的偏好，因而会产生不同的正则化效果，最常用的Ω函数有两种，即l1 范数和l2 范数，相应称之为L1 正则化和L2正则化

L1和L2范数，这两个范数正则化都有助于降低模型的过拟合风险，但前者L1范数还会带来另外一个好处，L1范数比L2范数更容易获得稀疏解，也就是它求的w会有更少的非零分量。

<img src="https://raw.githubusercontent.com/AnJian2020/study_recorder/main/images/202308271814014.png" alt="image-20230827181400928" style="zoom:67%;" />

L1范数比L2范数更容易获得稀疏解，也就是它求的**参数**会有更少的非零分量。

假设样本x只有两个属性，于是无论哪个公式求解出来的系数w仅有两个分量θ_1和θ_2。我们将其作为两个坐标轴，然后在图中绘制出这两个公式中第一项损失函数的等值线，蓝色线圈，再分别画出第二项范数的等值线，橙色线圈，就会出现两个等值线的交叉处。

从图中可以看出，采用L1范数的时候，两个等值线的交叉点出现在某个坐标轴上，也就是其中一个θ分量将为0，而采用L2范数的时候，交叉点常出现在某个象限内，θ的分量均不为0，换言之，采用L1范数比L2范数更容易得到稀疏解。

![image-20230827181501884](https://raw.githubusercontent.com/AnJian2020/study_recorder/main/images/202308271815976.png)

L1和L2正则化产生效果不同的原因：以L2范数正则化为例进行说明，假设图中椭圆为原目标函数J(w)的一条等高线，圆为半径sqrt(C)的范数球。由于约束条件的限制，w必须位于L2范数球内。考虑边界上的一点w，图中蓝色箭头为损失函数在该处的梯度方向∇J(w)，红色箭头为L2范数球在该处的法线方向。由于w不能离开边界（否则违反约束条件），因而在使用梯度下降法更新w时，只能朝∇J(w)在范数球上w处的切线方向更新，即图中绿色箭头的方向。如此w将沿着边界移动，当∇J(w)与范数球上w处的法线平行时，此时∇J(w)在切线方向的分量为0，将无法继续移动，从而达到最优解w*（图中红色点所示）。

<img src="https://raw.githubusercontent.com/AnJian2020/study_recorder/main/images/202308271817746.png" alt="image-20230827181757669" style="zoom:67%;" />

加了L2正则化项的线性回归模型称之为岭回归，加了L1正则化的线性回归模型称为Lasso回归。

<img src="https://raw.githubusercontent.com/AnJian2020/study_recorder/main/images/202308271819985.png" alt="image-20230827181940884" style="zoom:80%;" />

对比左右两张图，在岭回归中，惩罚因子是变量系数的平方值之和。惩罚因子缩小了自变量的系数，但从来没有完全消除它们。这意味着通过岭回归，你的模型中的噪声将始终被你的模型考虑在内。

而在LASSO正则化中，只需惩罚高系数特征，而不是惩罚数据中的每个特征。此外，LASSO能够将系数一直缩小到零。这相当于从数据集中删除这些特征，通过LASSO回归，模型有可能消除在数据集中的大部分噪声。这在某些情况下非常有用！

**岭回归**

在经验风险（损失函数）最小化的基础上加入正则化因子。当正则化因子选择为模型参数的二范数的时候，整个回归的方法就叫作岭回归。

![image-20230827182144438](https://raw.githubusercontent.com/AnJian2020/study_recorder/main/images/202308271821502.png)

**Lasso回归**

Lasso回归，翻译成中文叫套索，是通过构造一个惩罚函数得到一个较为精炼的模型，可以使得一些特征的系数直接变为0，从而增强模型的泛化能力。

![image-20230827182401960](https://raw.githubusercontent.com/AnJian2020/study_recorder/main/images/202308271824032.png)

**ElasticNet回归（弹性网络回归）**

即岭回归和Lasso技术的混合。弹性网络是一种使用 L1， L2 范数作为先验正则项训练的线性回归模型。 这种组合允许学习到一个只有少量参数是非零稀疏的模型，就像 Lasso 一样，但是它仍然保持一些像 Ridge 的正则性质。 弹性网络在很多特征互相联系的情况下是非常有用的。Lasso 很可能只随机考虑这些特征中的一个，而弹性网络更倾向于选择两个。
 当α接近0时，ElasticNet表现接近lasso， 当α从1变化到0时，目标函数的稀疏解（系数为0的情况）从0增加到Lasso的稀疏解。

<img src="https://raw.githubusercontent.com/AnJian2020/study_recorder/main/images/202308271825711.png" alt="image-20230827182543633" style="zoom:67%;" />

**总结**

- 误差与拟合：经验误差、泛化误差、过拟合、欠拟合
- 过拟合的解决方法：过拟合原因、方差、偏差、L1范数、L2范数、惩罚系数
- 线性回归的正则化：Lasoo回归、岭回归
