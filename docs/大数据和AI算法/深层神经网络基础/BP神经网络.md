## 单层神经网络实现

### 营尾花数据集分类实战

- 训练集：用于训练模型的数据集，模型在训练集上进行迭代学习，不断优化模型参数，以使其能够更好地拟合数据集。训练集通常是最大的数据集，占总数据集的大部分。
- 测试集：用于评估最终模型性能的数据集，模型在测试集上进行评估，以便在未见过的数据上验证其泛化能力。测试集通常是从数据集中单独保留的一部分数据，不能用于训练或调整模型。
- 验证集：用于调整模型参数和选择模型的数据集，模型在验证集上评估自己的性能，通过调整模型参数和选择最佳的模型，以使其在验证集上达到最佳的性能表现。验证集通常是从训练集中随机选择的一部分数据。

**神经网络的结构：单层前馈型神经网络**

> 单层前馈型神经网络，也称为感知机（Perceptron），是一种最简单的神经网络模型。它由一个输入层和一个输出层组成，其中每个输入节点与输出节点之间有权重连接。
>
> 在单层前馈型神经网络中，输入数据通过权重和激活函数的作用传递到输出层。每个输入节点与输出节点之间的连接都有一个权重，表示其对输出的影响程度。输出层根据输入节点的加权和经过激活函数的结果来生成最终的输出。
>
> 单层前馈型神经网络的结构非常简单，它无法处理复杂的非线性问题。它主要用于解决二分类问题，例如将输入数据分为两个类别。常见的激活函数包括阶跃函数和sigmoid函数。
>
> 尽管单层前馈型神经网络的拟合能力有限，但其具有一些简单问题的解决能力。并且它的训练过程相对简单，可以使用简单的规则（例如感知机学习规则）进行权重的更新。
>
> 然而，对于更复杂的问题，单层前馈型神经网络无法拟合非线性的决策边界。为了解决这个问题，引入了多层前馈型神经网络，如深度神经网络，它可以通过引入隐藏层和更复杂的激活函数来拟合更复杂的非线性关系。
>
> 总结来说，单层前馈型神经网络是一种简单的神经网络模型，由一个输入层和一个输出层组成。它主要用于解决简单的二分类问题，具有有限的拟合能力，并且训练过程相对简单。对于更复杂的问题，需要使用深度神经网络等更强大的模型。

**激活函数：softmax函数**

> Softmax函数是一种常用的激活函数，常用于多类别分类问题中的输出层。它可以将一组实数转换为表示概率分布的向量。
>
> 对于给定的输入向量 $x = [x_1, x_2, ..., x_n]$，Softmax函数的定义如下：
>
> $$\text{softmax}(x_i) = \frac{e^{x_i}}{\sum_{j=1}^{n} e^{x_j}}$$
> 其中，$e$ 是自然对数的底数（约等于2.71828）。
>
> Softmax函数的作用是将输入向量的每个元素进行指数运算，然后归一化得到一个概率分布。这样，输出向量的每个元素均为非负数，并且所有元素之和为1，可以看作是每个类别的概率估计。
>
> Softmax函数具有以下特点：
> 1. 归一化: Softmax函数将输入向量映射到概率分布上，确保输出的各个元素在0到1之间，并且总和为1。
> 2. 敏感度: Softmax函数对输入值的变化具有敏感度，较大的输入值会得到较大的输出概率，较小的输入值则会得到较小的输出概率。
> 3. 多类别分类: Softmax函数常用于多类别分类问题中，输出的概率分布表示样本属于每个类别的概率。
>
> 在神经网络的输出层中，Softmax函数通常与交叉熵损失函数（cross-entropy loss）一起使用，用于计算预测结果和真实标签之间的差异，并进行模型的训练和优化。
>
> 总结来说，Softmax函数是一种常用的激活函数，在多类别分类问题中常用于输出层。它将输入向量映射为一个概率分布，使得输出的每个元素均为非负数且总和为1。

**损失函数：交叉熵损失函数**		

> 交叉熵损失函数（Cross-Entropy Loss Function）是一种常用的损失函数，主要用于分类任务中衡量模型预测结果与真实标签之间的差距。它可以测量概率分布之间的相似性，并且在训练神经网络时能够有效地推动模型的学习过程。
>
> 对于二分类问题，交叉熵损失函数可以表示为：
> ```
> L(y, y') = -(y * log(y') + (1 - y) * log(1 - y'))
> ```
> 其中，`y` 是真实标签（0 或 1），`y'` 是模型预测的概率值。这个损失函数会惩罚模型对真实标签预测错误的情况，通过最小化损失来优化模型参数。
>
> 对于多类别分类问题，交叉熵损失函数是两个概率分布之间的交叉熵。对于真实标签的 one-hot 编码形式 `(y_1, y_2, ..., y_n)`，模型预测的概率分布为 `(y'_1, y'_2, ..., y'_n)`，交叉熵损失函数可以表示为：
> ```
> L(y, y') = -sum(y_i * log(y'_i))
> ```
> 其中，`sum` 是求和操作，`y_i` 和 `y'_i` 分别表示真实标签和模型预测的第 i 类的概率值。
>
> 交叉熵损失函数在神经网络的反向传播中起着重要的作用，它能够将梯度信息有效地传递回模型的参数，并进行参数更新，从而推动模型学习。
>
> 在 TensorFlow 中，可以使用 `tf.keras.losses.categorical_crossentropy` 或 `tf.keras.losses.BinaryCrossentropy` 函数来计算交叉熵损失。这些函数会根据真实标签和模型预测结果自动计算出交叉熵损失值，并返回一个张量表示损失。
>
> 总结来说，交叉熵损失函数是一种常用的损失函数，用于度量模型预测结果与真实标签之间的差异。它在分类任务中被广泛应用，通过最小化损失来优化模型参数。在 TensorFlow 中，可以使用相应的函数来计算交叉熵损失。

独热编码：tf.one_hot(indices, depth)

> `tf.one_hot` 是 TensorFlow 中的一个函数，用于将整数序列转换为独热编码（one-hot encoding）。独热编码是一种常用的表示分类变量的方法，在机器学习和深度学习中广泛应用。
>
> `tf.one_hot` 的语法如下：
> ```python
> tf.one_hot(
>     indices,
>     depth,
>     on_value=None,
>     off_value=None,
>     axis=None,
>     dtype=None,
>     name=None
> )
> ```
> 其中，`indices` 是一个整数张量，表示原始的整数序列。`depth` 是一个整数，表示独热编码的维度，即分类的总数。`on_value` 和 `off_value` 分别表示独热编码中对应原始整数和其他整数的值，默认为 `1` 和 `0`。`axis` 表示进行独热编码的维度，如果未指定，则默认为最后一个维度。`dtype` 表示输出的数据类型，默认为 `tf.float32`。
>

## 多层神经网络实现