RCNN， Fast-RCNN，Faster-RCNN是基于深度学习的分类方法。

YOLO系列是基于深度学习的回归方法，训练和检测均是在一个单独网络中进行，没有显示地求取region proposal的过程。

<img src="https://raw.githubusercontent.com/AnJian2020/study_recorder/main/images/202308140910292.png" alt="image-20230814091042063" style="zoom:67%;" />

## R-CNN

RCNN 通常指的是区域卷积神经网络（Region-based Convolutional Neural Network）。

传统的 RCNN 模型由三个主要组件组成：

1. 区域提取（Region Proposal）：首先使用选择性搜索（Selective Search）等算法从输入图像中提取候选区域。这些候选区域是潜在的目标物体位置。
2. 特征提取（Feature Extraction）：对每个候选区域应用预训练的卷积神经网络（如AlexNet或VGGNet），将其转换为固定长度的特征向量。
3. 目标分类和定位（Object Classification and Localization）：将每个候选区域的特征向量输入到分类器中，同时进行目标分类和边界框回归，以确定目标物体的类别和准确位置。

<img src="https://raw.githubusercontent.com/AnJian2020/study_recorder/main/images/202308140916715.png" alt="image-20230814091656556" style="zoom:50%;" />

**存在的问题**

- 需要特定的训练目标
- 训练耗时长，花费大量的磁盘空间
- 推理(检测)很慢

## Fast R-CNN

Faster R-CNN 是一种改进的 RCNN（Region-based Convolutional Neural Network）模型，旨在解决 RCNN 模型中的计算量大和速度慢的问题。Faster R-CNN 引入了区域提案网络（Region Proposal Network，RPN），将目标检测任务中的区域提案与分类和定位任务统一在一个端到端的神经网络中进行。

![img](https://raw.githubusercontent.com/AnJian2020/study_recorder/main/images/202308140934650.webp)

Faster R-CNN的主要内容：

1. Conv layers。作为一种CNN网络目标检测方法，Faster RCNN首先使用一组基础的conv+relu+pooling层提取image的feature maps。该feature maps被共享用于后续RPN层和全连接层。
2. Region Proposal Networks。RPN网络用于生成region proposals。该层通过softmax判断anchors属于positive或者negative，再利用bounding box regression修正anchors获得精确的proposals。
3. Roi Pooling。该层收集输入的feature maps和proposals，综合这些信息后提取proposal feature maps，送入后续全连接层判定目标类别。
4. Classification。利用proposal feature maps计算proposal的类别，同时再次bounding box regression获得检测框最终的精确位置。

![image-20230814093622170](https://raw.githubusercontent.com/AnJian2020/study_recorder/main/images/202308140936305.png)

![image-20230814093637229](https://raw.githubusercontent.com/AnJian2020/study_recorder/main/images/202308140936357.png)

## YOLO模型——目标检测

直接用整张原图训练一个CNN构成完整的检测管道，最终通过直接回归预测目标的位置和类别，是One-stage，端到端的目标检测和识别。没有anchor、多尺度。

### 基本思路

<img src="https://raw.githubusercontent.com/AnJian2020/study_recorder/main/images/202308140939206.png" alt="image-20230814093903125" style="zoom:50%;" />

对原图划分网格（5x5），每个格子同时预测 B个检测框（2个，手工选择的）及每个框的置信度，以及C个条件类别概率（VOC 20 个）。

### 网络结构

- YOLO的最终损失是将定位损失、框置信度损失和分类损失加在一起。他们都是使用预测值和GT之间的误差平方和来计算损失。

- 训练有两个阶段：

  ​	首先:预训练分类器网络。（224x224） 

  ​	然后:用卷积层替换全连接层，并端到端地重新训练目标检测网络。 （448x448）

<img src="https://raw.githubusercontent.com/AnJian2020/study_recorder/main/images/202308140944791.png" alt="image-20230814094403663" style="zoom:50%;" />

![image-20230814094425001](https://raw.githubusercontent.com/AnJian2020/study_recorder/main/images/202308140944075.png)

![image-20230814094654604](https://raw.githubusercontent.com/AnJian2020/study_recorder/main/images/202308140946717.png)

### 缺点

- 检测时输入尺寸固定，没有用多尺度的特征输入。小目标检测效果差
- 同一格子包含多个目标时，每个网格单元仅预测一个目标，漏检很多
- YOLO默认落入同一格子里的所有边界框均为同种类的目标
- 损失函数中IOU的loss没有区分大物体IOU和小物体IOU的误差对于网络训练loss的影响，降低了定位准确性，实际上小物体IOU的误差对网络优化过程会造成更大的影响
- 同一物体出现不常见的长宽比等情况时，效果较差
- 速度更快，但精度较低