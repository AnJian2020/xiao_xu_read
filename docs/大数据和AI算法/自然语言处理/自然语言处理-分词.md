## 分词概述

词是自然语言中能够独立运用的最小单位，是语言信息处理的基本单位。 

词法分析是对自然语言的形态(morphology) 进行分析，判断词的结构、类别和性质。 

主要任务包括： 

- 自动分词（segmentation） 
- 命名实体识别（Named Entity Recognition）
- 词性标注（Part-of-Speech）

**分词算法**

1. 基于规则的分词方法。
   - 基本思想：按照一定策略将待分析的汉字串与一个“充分大的”机器词典中的词条进行匹配
   - 根据扫描方向的不同，这种分词方法分为正向匹配、逆向匹配和 双向匹配。
   - 常见的匹配原则有逐词匹配、最大匹配、最小匹配和最佳匹配。
   - 简单易行，但歧义消解能力差
2. 基于统计的分词方法
   - 基本思想：在分词的同时进行句法、语义分析，利用句法信息和语义信息来处理歧义现象，让计算机模拟人对句子的理解来进行分词。
   - 主要方法：N 元文法模型（N-gram）、隐马尔可夫模型（Hiden Markov Model，HMM）、最大熵模型（ME）、条件随机场模型（Conditional Random Fields，CRF）等
   - 效果依赖于训练语料的规模和质量
3. 基于理解的分词方法
   - 基本思想：在分词的同时进行句法、语义分析，利用句法信息和语义信息来处理歧义现象，让计算机模拟人对句子的理解来进行分词。
   - 主要方法：专家系统分词、神经网络分词（LSTM,CNN）
   - 需要大量的语言知识和信息

## 基于规则的分词方法

基于规则的分词方法，又称为机械式分词方法、基于字典的分词方法。

按照一定策略，将待分析的汉字与一个“极大的”机器词典中的词条进行匹配。若在词典中找到某个字符串，则匹配成功。

根据扫描方向的不同，分为正向匹配、逆向匹配和双向匹配。

根据匹配原则的不同，分为逐词匹配、最大匹配、最小匹配和最佳匹配

### 逐词匹配法

### 正向最大匹配法

正向最大匹配法（Maximum MatchingMethod），简称为 MM 法，基于词典的分词方法。

对于输入的一段文本从左至右、以贪心的方式切分出当前位置上长度最大的词。

> 分词原理：单词的颗粒度越大，所能表示的含义越确切。

**主要步骤**

1. 一般从一个字符串的开始位置，选择一个最大长度的词长的片段，如果序列不足最大词长，则选择全部序列。 
2. 首先看该片段是否在词典中，如果是，则算为一个分出来的词，如果不是，则从右边开始，减少一个字符， 然后看短一点的这个片段是否在词典中，依次循环，逐到只剩下一个字。
3. 序列变为第2步骤截取分词后，剩下的部分序列。

<img src="https://raw.githubusercontent.com/AnJian2020/study_recorder/main/images/202308141603017.png" alt="image-20230814160315878" style="zoom: 50%;" />

**示例**

```python
test_file = 'train.txt'  # 训练语料
test_file2 = 'test.txt'  # 测试语料
test_file3 = 'test_sc.txt'  # 生成结果


def get_dic(test_file):  # 读取文本返回列表
    with open(test_file, 'r', encoding='utf-8', ) as f:
        try:
            file_content = f.read().split()
        finally:
            f.close()
    chars = list(set(file_content))
    return chars


def word_match(test_file2):
    h = open(test_file3, 'w', encoding='utf-8', )
    with open(test_file2, 'r', encoding='utf-8', ) as f:
        lines = f.readlines()
    for line in lines:  # 分别对每行进行正向最大匹配处理
        max_length = 5
        my_list = []
        len_hang = len(line)
        while len_hang > 0:
            tryWord = line[0:max_length]
            while tryWord not in dic:
                if len(tryWord) == 1:
                    break
                tryWord = tryWord[0:len(tryWord) - 1]
            my_list.append(tryWord)
            line = line[len(tryWord):]
            len_hang = len(line)
        for t in my_list:  # 将分词结果写入生成文件
            if t == '\n':
                h.write('\n')
            else:
                h.write(t + "-")
    h.close()


if __name__ == "__main__":
    dic = get_dic(test_file)
    readfile(test_file2)
```



### 反向最大匹配法

反向最大匹配法（Reverse Maximum Matching Method），简称为 RMM法，其基本原理与正向最大匹配法类似，只是分词顺序变为从右至左，误差 比正向最大匹配法要小。

由于汉语中偏正结构较多，若从后向前匹配，可以适当提高精确度。 

它使用的分词词典是逆序词典，其中的每个词条都将按逆序方式存放。在实际处理时，先将文档进行倒排处理，生成逆序文档。

<img src="https://raw.githubusercontent.com/AnJian2020/study_recorder/main/images/202308141701939.png" alt="image-20230814170123807" style="zoom:50%;" />

**示例**

```python
def get_dic(test_file):
    with open(test_file, 'r', encoding='utf-8') as f:
        file_content = f.read().split()
    chars = list(set(file_content))
    return chars


def reverse_word_match(test_file2, test_file3, dic):
    max_length = 5
    word = open(test_file3, 'w', encoding='utf-8')  # 分词结果
    with open(test_file2, 'r', encoding="utf-8") as f:
        lines = f.readlines()
    for line in lines:
        tmp_word_stack = []
        line_len = len(line)
        while line_len > 0:
            tmp_str = line[-max_length:]
            while tmp_str not in dic:
                if len(tmp_str) == 1:
                    break
                tmp_str = tmp_str[1:]
            tmp_word_stack.append(tmp_str)
            line = line[0:len(line)-len(tmp_str)]
            line_len = len(line)
        while len(tmp_word_stack):
            tmp_word = tmp_word_stack.pop()
            if tmp_word=='\n':
                word.write("\n")
            else:
                word.write(tmp_word+"--")
    word.close()

if __name__ == "__main__":
    test_file = 'train.txt'  # 训练语料
    test_file2 = 'test.txt'  # 测试语料
    test_file3 = 'test_sc_reverse.txt'  # 生成结果
    dic = get_dic(test_file)
    reverse_word_match(test_file2, test_file3, dic)
```

### 双向最大匹配法

双向最大匹配法是将正向最大匹配法得到的分词结果和逆向最大匹配法的到的结果进行比较，从而决定正确的分词方法。

**规则**

1. 如果正反向分词结果词数不同，则取分词数量较少的那个。
2. 如果分词结果词数相同
   - 分词结果相同，就说明没有歧义，可返回任意一个。
   - 分词结果不同，返回其中单字较少的那个。

```python
class TokenizerTool(object):
    def __init__(self, word_file):
        self.max_length = 3
        self.words_dict = self.get_dic(word_file)

    def get_dic(self, words_file):
        with open(words_file, 'r', encoding='utf-8') as f:
            file_content = f.read().split()
        word_dict_set = list(set(file_content))
        return word_dict_set

    def mm_cut(self, text):
        result = []
        start = 0
        text_length = len(text)
        while text_length > start:
            for end in range(self.max_length + start, start, -1):
                piece = text[start:end]
                if piece in self.words_dict or len(piece) == 1:
                    start = end
                    break
            result.append(piece)
        print(f"mm result:{result}")
        return result

    def rmm_cut(self, text):
        result = []
        start = len(text)
        while start > 0:
            for end in range(start - self.window_size, start, 1):
                piece = text[end:start]
                # print(f'end:{end},start:{start},piece:{piece}')
                if piece in self.dic or len(piece) == 1:
                    start = end
                break
            result.append(piece)
        result.reverse()
        print(f'rmm result : {result}')
        return result

    def get_words_len_dict(self, words_lst):
        w_len_dict = {}
        for word in words_lst:
            w_len = len(word)
            if w_len in w_len_dict.keys():
                w_len_dict[w_len] = w_len_dict[w_len] + 1
            else:
                w_len_dict[w_len] = 0
        return w_len_dict

    def dmm_cut(self, text):
        mm_toks = self.mm_cut(text)

        rmm_toks = self.rmm_cut(text)
        if len(mm_toks) < len(rmm_toks):
            print(f'dmm result : {mm_toks}')
            return mm_toks
        elif len(mm_toks) > len(rmm_toks):
            print(f'dmm result : {rmm_toks}')
            return rmm_toks
        else:
            mm_w_len_dict = self.get_words_len_dict(mm_toks)
            rmm_w_len_dict = self.get_words_len_dict(rmm_toks)
            if mm_w_len_dict[1] < rmm_w_len_dict[1]:
                print(f'dmm result : {mm_toks}')
                return mm_toks
            else:
                print(f'dmm result : {rmm_toks}')
                return rmm_toks


if __name__ == '__main__':
    text = '研究生命的起源'
    tokenizer = TokenizerTool('words.txt')
    mm_res = tokenizer.mm_cut(text)
    rmm_res = tokenizer.rmm_cut(text)
    tokenizer.dmm_cut(text)

```



### 常用的分词工具

常用的分词工具有：jieba分词、StanfordCoreNLP、HanLP、THULAC、SnowNLP、NLPIR。

#### jieba分词工具

jieba 支持三种分词模式：精确模式、全模式和搜索引擎模式。

- 精确模式：试图将语句最精确的切分，不存在冗余数据，适合做文本分析
- 全模式：将语句中所有可能是词的词语都切分出来，速度很快，但是存在冗余数据
- 搜索引擎模式：在精确模式的基础上，对长词再次进行切分

**示例**

```python
import jieba

sent = "中文分词是文本处理不可或缺的一步！"

seg_list = jieba.cut(sent, cut_all=False)
print("精确模式：", '/'.join(seg_list))

seg_list = jieba.cut(sent, cut_all=True)
print("全模式：", '/'.join(seg_list))

# 默认精确模式
seg_list = jieba.cut(sent)
print("默认模式：", '/'.join(seg_list))

seg_list = jieba.cut_for_search(sent)
print("搜索模式：", '/'.join(seg_list))
```