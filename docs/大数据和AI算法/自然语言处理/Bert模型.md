## 简介

BERT的全称是Bidirectional Encoder Representation from Transformers，通过在大规模无标签文本数据上进行预训练，学习到了丰富的语言表示，并且可以应用于多种下游自然语言处理任务。

<img src="https://raw.githubusercontent.com/AnJian2020/study_recorder/main/images/202308151651776.png" alt="image-20230815165133515" style="zoom:50%;" />

**特点**

1. 双向性：BERT模型采用了双向Transformer结构，能够同时考虑上下文信息，从而更好地捕捉词汇的含义。
2. 预训练+微调：BERT模型分为两个阶段，首先在大规模无标签文本数据上进行预训练，学习通用的语言表示，然后再在特定任务上进行微调，以适应不同的下游任务。
3. 掩码语言模型：在预训练阶段，BERT模型通过随机掩盖一部分输入文本中的单词，然后预测被掩盖的单词是什么，从而使模型学会理解上下文中的词汇。
4. 下游任务微调：在微调阶段，BERT模型将预训练的权重加载到一个特定任务的模型结构中，并在有标签的数据上进行训练以优化模型在该任务上的性能。

### 句子分类

使用BERT最简单的方法就是做一个文本分类模型，这样的模型结构如下图所示。

<img src="https://raw.githubusercontent.com/AnJian2020/study_recorder/main/images/202308151653251.png" alt="image-20230815165357136" style="zoom:50%;" />

### 词嵌入

词嵌入的常见方法：Word2vec和Gloved等。

1. Word Embedding Recap

   - 为了让机器可以学习到文本的特征属性，需要一些将文本数值化的表示的方式。Word2vec算法通过使用一组固定维度的向量来表示单词，计算其方式可以捕获到单词的语义及单词与单词之间的关系。
   - 使用Word2vec的向量化表示方式可以用于判断单词是否相似、对立，

2. ELMo(Embeddings from Language Models)

   - 词嵌入方式问题：因为使用预训练好的词向量模型，无论上下文的语境关系如何，每个单词都只有唯一的且已经固定保存的向量化形式.
   - 解决一词多义的问题
   - 例如， ‘长’ 在 ‘长度’ 这个词中表示度量，在 ‘长高’ 这个词中表示增加。那么通过’长‘周围是 度 或者是 高 ，来判断它的读音或者它的语义，这个问题就派生出语境化的词嵌入模型。
   - ELMo改变Word2vec类的将单词固定为指定长度的向量的处理方式，它是在为每个单词分配词向量之前先查看整个句子，然后使用biLSTM来训练它对应的词向量
   - ELMo的LSTM可以使用与任务相 关的大量文本数据来进行训练，然后将训练好的模型用作其他NLP任 务的词向量的基准
   - ELMo会训练一个模型，这个模型接受一个句子或者单词的输入，输出最有可能 出现在后面的一个单词。 
   - 模型很容易实现拥有大量的文本数据， 可以在不需要标签的情况下去学习。
   -  任务：输入“Lets stick to”，预测下一个最 可能出现的单词
   - 如果在训练阶段使用大量的数据集进行训练，那么在预测阶段可能准确的预测出期待的下一个单词。比如输入“机器”，在‘’学习‘和‘买菜’ 中它最有可能的输出会是‘学习’而不是‘买菜’。

   <img src="https://raw.githubusercontent.com/AnJian2020/study_recorder/main/images/202308151702434.png" alt="image-20230815170241257" style="zoom:50%;" />

