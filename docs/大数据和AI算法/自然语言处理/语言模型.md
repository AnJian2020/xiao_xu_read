## 语言模型的发展

<img src="https://raw.githubusercontent.com/AnJian2020/study_recorder/main/images/202308150959187.png" alt="image-20230815095909963" style="zoom:50%;" />

## 基础概念

一个语言模型通常构建为字符串s的概率分布p(s)，这里的p(s)实际上反映的是s作为一个句子出现概率。概率指的是组成字符串的这个组合，在训练语料中出现的似然，与句子是否合乎语法无关。假设训练语料来自于人类的语言，可以认为这个概率是的是一句话是否通顺的概率。

语言模型形式化的描述就是给定一个字符串，看它是自然语言的概率 P(w1,w2,…,wt)。w1 到 wt 依次表示这句话中的各个词。
$$
P(w_1,w_2,…,w_t)=P(w_1)×P(w_2|w_1)×P(w_3|w_1,w_2)×…×P(w_t|w_1, w_2,…,w_{t−1})
$$
一个统计语言模型可以表示成，给定前面的的 词，求后面一个词出现的条件概率

**示例**

![image-20230815101042497](https://raw.githubusercontent.com/AnJian2020/study_recorder/main/images/202308151010611.png)

**存在的问题**

1. 自由参数数目
   - 假定字符串中字符全部来自与大小为V的词典，上述例子中我们需要计算所有的条件概率，对于所有的条件概率，这里的w都有V种取值，那么实际上这个模型的自由参数数目量级是V^6，6为字符串的长度。 
   - 从上面可以看出，模型的自由参数是随着字符串长度的增加而指数级暴增的，这使我们几乎不可能正确的估计出这些参数。
2. 数据稀疏性
   - 从上面可以看到，每一个w都具有V种取值，这样构造出了非常多的词对，但实际中训练语料是不会出现这么多种组合的，那么依据最大似然估计，最终得到的概率实际是很可能是0。

### 求解方法

#### n-gram模型

为了解决自由参数数目过多的问题，引入了马尔科夫假设：随意一个词出现的概率只与它前面出现的有限的n个词有关。基于上述假设的统计语言模型被称为N-gram语言模型。

n-gram模型为了保持词的顺序，做了一个滑窗的操作，这里的n表示的就是滑窗的大小。（模型考虑了词的顺序）

> 通常情况下，n的取值不能够太大，否则自由参数过多的问题依旧存在：
>
> 1. 当n=1时，即一个词的出现与它周围的词是独立，这种我们称为unigram，也就是一元语言模型，此时自由参数量级是词典大小V。
> 2. 当n=2时，即一个词的出现仅与它前面的一个词有关时，这种我们称为bigram，叫二元语言模型，也叫一阶马尔科夫链，此时自由参数数量级是V^2。
> 3. 当n=3时，即一个词的出现仅与它前面的两个词有关，称为trigram，叫三元语言模型，也叫二阶马尔科夫链，此时自由参数数量级是V^3

自由参数的数量级是n取值的指数倍。从模型的效果来看，理论上n的取值越大，效果越好。但随着n取值的增加，效果提升的幅度是在下降的。 

涉及到可靠性和可区别性的问题，参数越多，可区别性越好，但同时单个参数的实例变少从而降低了可靠性。

**例**：2-gram模型，也就是把2个词当做一组来处理，然后向后 移动一个词的长度，再次组成另一组词，把这些生成一个字典， 按照词袋模型的方式进行编码得到结果。

**缺点**：随着n的大小增加，词表会成指数型膨胀，会越来越大。

**存在的问题(离散表示存在的问题)**

- 由于存在以下的问题，对于一般的NLP问题，是可以使用离散表示文本信息来解决问题的，但不适用于要求精度较高的场景。

- 无法衡量词向量之间的关系。

- 词表的维度随着语料库的增长而膨胀。

- n-gram词序列随语料库增长呈指数型膨胀，更加快。

- 离散数据来表示文本会带来数据稀疏问题，导致丢失了信息，与我们生活中理解的信息是不一样的。

#### 神经概率语言模型

神经概率语言模型（Neural Probabilistic Language Model）是一种基于神经网络的语言模型，用于估计句子或文本序列出现的概率。它通过学习词嵌入表示和上下文之间的复杂关系，从而能够更好地捕捉句子的语义和上下文信息。

神经概率语言模型通常包含以下几个关键组件：

1. 输入表示（Input Representation）：将输入的词或字符转化为向量表示，常用的方法包括词嵌入（Word Embedding）和字符嵌入（Character Embedding）。
2. 神经网络结构（Neural Network Architecture）：神经概率语言模型使用神经网络来建模句子的语言结构和上下文关系。常见的结构包括循环神经网络（Recurrent Neural Network，RNN），长短期记忆网络（Long Short-Term Memory，LSTM）和Transformer等。
3. 上下文建模（Context Modeling）：神经概率语言模型通过上下文建模来捕捉句子中词与词之间的依赖关系。它利用先前的词向量或隐藏状态来预测当前词的出现概率。
4. 输出层（Output Layer）：输出层将上下文建模得到的表示映射为词汇表上每个词的概率分布。常见的方法包括使用softmax函数或非归一化的对数线性模型（log-linear model）进行计算。
5. 训练和优化（Training and Optimization）：神经概率语言模型通过最大似然估计等方法，在大规模语料库上进行训练。优化算法如随机梯度下降（Stochastic Gradient Descent，SGD）和自适应优化算法（Adam，Adaptive Moment Estimation）用于调整网络参数以最大化语言模型的概率估计准确性。