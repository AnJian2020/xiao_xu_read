循环神经网络（Recurrent Neural Network，RNN）是一种在序列数据上进行操作的神经网络。

**关键思想**：引入循环连接，使得网络的隐藏状态能够在每个时间步骤上被传递，并影响网络的后续输出。这种循环连接允许 RNN 对序列中的先前信息进行建模，从而更好地理解和预测序列数据。

在 RNN 中，每个时间步骤上的输入和隐藏状态都会影响下一个时间步骤的隐藏状态。这种内部状态的传递使 RNN 能够对不同长度和特征的序列数据进行建模，例如自然语言文本、音频信号或时间序列数据。

> - 使用带自反馈的神经元，处理任意长度序列。
>
> - 更加符合生物神经网络结构

给定一个输入序列
$$
x=(x_1,x_2,…,x_t)
$$
，通过公式1更新带反馈边的隐藏层的活性值h_t。

<img src="https://raw.githubusercontent.com/AnJian2020/study_recorder/main/images/202308132106834.png" alt="image-20230813210643257" style="zoom:50%;" />

<img src="https://raw.githubusercontent.com/AnJian2020/study_recorder/main/images/202308132109709.png" alt="image-20230813210914708" style="zoom:50%;" />

<img src="https://raw.githubusercontent.com/AnJian2020/study_recorder/main/images/202308132109517.png" alt="image-20230813210959407" style="zoom:50%;" />

**特点**

- 时间反向传播算法来学习参数
- 存在梯度爆炸、消失问题
- 较长的记忆无法产生作用

**参数说明**

 `tf.keras.layers.SimpleRNN` 层的主要参数说明：

- `units`：整数，输出空间的维度，即 SimpleRNN 层中的神经元数量。
- `activation`：激活函数，默认为双曲正切（tanh）。可以是内置的激活函数名称，也可以是可调用的对象。常见的激活函数包括 sigmoid、relu 等。
- `use_bias`：布尔值，是否使用偏置向量，默认为 True。如果为 False，则模型不会使用偏置。
- `kernel_initializer`：权重矩阵的初始化方法，默认为 "glorot_uniform"，也可以是其他内置的初始化器或自定义的初始化器。
- `recurrent_initializer`：循环权重的初始化方法，默认为 "orthogonal"，也可以是其他内置的初始化器或自定义的初始化器。
- `bias_initializer`：偏置向量的初始化方法，默认为 "zeros"，也可以是其他内置的初始化器或自定义的初始化器。
- `kernel_regularizer`：权重矩阵的正则化方法，默认为 None。可以是内置的正则化方法，例如 L1 或 L2 正则化，也可以是自定义的正则化方法。
- `recurrent_regularizer`：循环权重的正则化方法，默认为 None。可以是内置的正则化方法，例如 L1 或 L2 正则化，也可以是自定义的正则化方法。
- `bias_regularizer`：偏置向量的正则化方法，默认为 None。可以是内置的正则化方法，例如 L1 或 L2 正则化，也可以是自定义的正则化方法。
- `activity_regularizer`：输出的正则化方法，默认为 None。可以是内置的正则化方法，例如 L1 或 L2 正则化，也可以是自定义的正则化方法。
- `dropout`：在输入上设置输入单元的丢弃比例（0-1之间的浮点数），默认为 0，表示不使用 Dropout。
- `recurrent_dropout`：在循环层状态之间设置循环单元的丢弃比例（0-1之间的浮点数），默认为 0，表示不使用 Dropout。
- `return_sequences`：布尔值，确定是否返回完整的输出序列，默认为 False。如果为 True，则返回完整的输出序列；如果为 False，则只返回最后一个时间步的输出。
- `return_state`：布尔值，确定是否返回最后一个时间步的状态，默认为 False。如果为 True，则除了输出之外，还将返回最后一个时间步的状态。

**示例**

```python
import tensorflow as tf
import numpy as np

inputs = np.random.random([4, 4, 8]).astype(np.float32)
print(inputs)
simple_rnn = tf.keras.layers.SimpleRNN(4)
output = simple_rnn(inputs)
print('output:', output)
simple_rnn1 = tf.keras.layers.SimpleRNN(4, return_sequences=True, return_state=True)
whole_sequence_output, final_state = simple_rnn1(inputs)
print('whole_sequence_output:', whole_sequence_output)
print('final_state:', final_state)
```

