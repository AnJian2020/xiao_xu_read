双向循环神经网络（Bidirectional Recurrent Neural Network, BRNN）是一种具有额外时间维度上的正向和反向传播的循环神经网络结构。它能够同时考虑当前时刻之前和之后的上下文信息，从而更好地捕捉序列数据中的特征和依赖关系。

<img src="https://raw.githubusercontent.com/AnJian2020/study_recorder/main/images/202308131048966.png" alt="image-20230813104840721" style="zoom:50%;" />

双向循环神经网络按时刻展开的结构，可以看到向前和向后层共同连接着输出层，包含6个共享权值，分别为输入到向前层和向后层两个权值、向前层和向后层各自隐含层到隐含层的权值、向前层和向后层各自隐含层到输出层的权值。

<img src="https://raw.githubusercontent.com/AnJian2020/study_recorder/main/images/202308131050590.png" alt="image-20230813105040492" style="zoom:50%;" />

**前向传播**

1. 沿着时刻1到时刻T正向计算一遍，得到并保存每个时刻向前隐含层的输出。

2. 沿着时刻T到时刻1反向计算一遍，得到并保存每个时刻向后隐含层的输出。

3. 正向和反向都计算完所有输入时刻后，每个时刻根据向前向后隐含层得到最终输出。

**反向传播** 

1. 计算所有时刻输出层的δ项。 

2. 根据所有输出层的δ项，使用 BPTT 算法更新向前层。

3. 根据所有输出层的δ项，使用 BPTT 算法更新向后层。

**工作原理**

1. 输入序列通过正向循环层进行前向传播，生成隐藏状态序列，得到正向的输出信息。
2. 输入序列通过反向循环层进行后向传播，生成反向的隐藏状态序列，得到反向的输出信息。
3. 正向和反向的输出信息可以按元素位置进行拼接或其他组合操作，形成最终的输出表示。

`tf.keras.layers.Bidirectional` 的主要参数如下：

- `layer`：用于包装的循环层。可以是一个循环层类的实例（如 `tf.keras.layers.LSTM`、`tf.keras.layers.GRU` 或 `tf.keras.layers.SimpleRNN`），或者是从 `tf.keras.layers.RNN` 派生的自定义循环层。
- `merge_mode`：指定如何合并正向和反向输出的方式。默认值为 "concat"，表示将正向和反向输出在特征维度上进行拼接。其他可能的值有 "sum"（元素求和）、"mul"（元素相乘）和 "ave"（元素求平均）。如果为None，则不合并输出，它们将作为列表返回。
- `weights`：可选参数，指定初始化权重的方式。如果为 None，则使用默认的参数初始化方法。
- `name`：可选参数，设置该层的名称。

```python
import tensorflow as tf
import numpy as np

data = np.array([0.1, 0.2, 0.4]).reshape((1, 3, 1))
input_data = tf.keras.Input(shape=(3, 1))
model = tf.keras.Sequential([tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(2, return_sequences=True,kernel_initializer=tf.ones_initializer(),recurrent_initializer=tf.ones_initializer()),input_shape=(3, 1)),
    # tf.keras.layers.Dense(4)
])
result = model.predict(data)
print(result)
print(result.shape)
```

